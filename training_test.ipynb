{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior4WeatherDetection Training Test\n",
    "\n",
    "This notebook tests the Prior-aware Adversarial Domain Adaptation for Object Detection under Adverse Weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from models.detection_network import DomainAdaptiveFasterRCNN\n",
    "from utils.data.cityscapes_clean_dataset import Cityscapes_Clean_Dataset, cityscapes_clean_dataset_collate_fn\n",
    "from utils.data.cityscapes_foggy_dataset import Cityscapes_Foggy_Dataset, cityscapes_foggy_dataset_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DomainAdaptiveFasterRCNN(\n",
       "  (backbone): CustomVGGBackbone(\n",
       "    (backbone_c4): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (backbone_c5): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (detector): FasterRCNN(\n",
       "    (transform): GeneralizedRCNNTransform(\n",
       "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "        Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "    )\n",
       "    (backbone): CustomVGGBackbone(\n",
       "      (backbone_c4): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (8): ReLU(inplace=True)\n",
       "        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): ReLU(inplace=True)\n",
       "        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (13): ReLU(inplace=True)\n",
       "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (15): ReLU(inplace=True)\n",
       "        (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (18): ReLU(inplace=True)\n",
       "        (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (20): ReLU(inplace=True)\n",
       "        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (22): ReLU(inplace=True)\n",
       "        (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (backbone_c5): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (rpn): RegionProposalNetwork(\n",
       "      (anchor_generator): AnchorGenerator()\n",
       "      (head): RPNHead(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (cls_logits): Conv2d(512, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bbox_pred): Conv2d(512, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (roi_heads): RoIHeads(\n",
       "      (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "      (box_head): TwoMLPHead(\n",
       "        (fc6): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "        (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (box_predictor): FastRCNNPredictor(\n",
       "        (cls_score): Linear(in_features=1024, out_features=10, bias=True)\n",
       "        (bbox_pred): Linear(in_features=1024, out_features=40, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pen_c4): PriorEstimationNetwork(\n",
       "    (conv1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (pen_c5): PriorEstimationNetwork(\n",
       "    (conv1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv4): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (rfrb_c4): ResidualFeatureRecoveryBlock(\n",
       "    (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (rfrb_c5): ResidualFeatureRecoveryBlock(\n",
       "    (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = DomainAdaptiveFasterRCNN(num_classes=10, backbone_name='vgg16')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer (SGD as in paper)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2975 images in train split.\n",
      "Found 5676 foggy images in train split for beta levels [0.01, 0.02, 0.05].\n"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "source_dataset = Cityscapes_Clean_Dataset(\"/teamspace/studios/this_studio/Prior4WeatherDetection/datataset/cityscapes\")  # Clean images + labels\n",
    "target_dataset = Cityscapes_Foggy_Dataset(\"/teamspace/studios/this_studio/Prior4WeatherDetection/datataset/cityscapes\")  # Hazy/Rainy images + priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dataset-specific collate functions\n",
    "target_loader = DataLoader(\n",
    "    target_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=cityscapes_foggy_dataset_collate_fn\n",
    ")\n",
    "\n",
    "source_loader = DataLoader(\n",
    "    source_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=cityscapes_clean_dataset_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source batch size: 2\n",
      "Target batch size: 2\n"
     ]
    }
   ],
   "source": [
    "# Testing a single batch pass\n",
    "model.train()\n",
    "\n",
    "for source_batch, target_batch in zip(source_loader, target_loader):\n",
    "    # Unpack source batch\n",
    "    source_images, source_prior_images, source_targets = source_batch\n",
    "    source_images = [img.to(device) for img in source_images]\n",
    "    source_prior_images = [img.to(device) for img in source_prior_images]\n",
    "    source_targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in source_targets]\n",
    "    \n",
    "    # Convert lists to tensors for batch processing\n",
    "    source_images_tensor = torch.stack(source_images)\n",
    "    source_prior_images_tensor = torch.stack(source_prior_images)\n",
    "\n",
    "    # Unpack target batch\n",
    "    target_images, target_prior_images, target_targets = target_batch\n",
    "    target_images = [img.to(device) for img in target_images]\n",
    "    target_prior_images = [img.to(device) for img in target_prior_images]\n",
    "    target_targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in target_targets]\n",
    "    \n",
    "    # Convert lists to tensors for batch processing\n",
    "    target_images_tensor = torch.stack(target_images)\n",
    "    target_prior_images_tensor = torch.stack(target_prior_images)\n",
    "\n",
    "    # Print batch sizes\n",
    "    print(\"Source batch size:\", len(source_images))\n",
    "    print(\"Target batch size:\", len(target_images))\n",
    "    \n",
    "    # Forward pass - source domain\n",
    "    try:\n",
    "        source_losses = model(source_images_tensor, source_prior_images_tensor, source_targets)\n",
    "        print(\"Source domain forward pass successful!\")\n",
    "        print(\"Source losses:\", source_losses)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in source domain forward pass: {e}\")\n",
    "    \n",
    "    # Forward pass - target domain\n",
    "    try:\n",
    "        target_losses = model(target_images_tensor, target_prior_images_tensor, target_targets)\n",
    "        print(\"Target domain forward pass successful!\")\n",
    "        print(\"Target losses:\", target_losses)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in target domain forward pass: {e}\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_epoch(model, source_loader, target_loader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    source_epoch_loss = 0.0\n",
    "    target_epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Alternate between source and target batches\n",
    "    for source_batch, target_batch in zip(source_loader, target_loader):\n",
    "        # ----- Source Domain Training -----\n",
    "        # Unpack source batch\n",
    "        source_images, source_prior_images, source_targets = source_batch\n",
    "        source_images = [img.to(device) for img in source_images]\n",
    "        source_prior_images = [img.to(device) for img in source_prior_images]\n",
    "        source_targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in source_targets]\n",
    "        \n",
    "        # Convert lists to tensors\n",
    "        source_images_tensor = torch.stack(source_images)\n",
    "        source_prior_images_tensor = torch.stack(source_prior_images)\n",
    "        \n",
    "        # Forward pass for source domain\n",
    "        source_losses = model(source_images_tensor, source_prior_images_tensor, source_targets)\n",
    "        \n",
    "        # Calculate total source loss\n",
    "        source_loss = source_losses['loss_classifier'] + source_losses['loss_box_reg'] + \\\n",
    "                    source_losses['loss_objectness'] + source_losses['loss_rpn_box_reg']\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        source_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        source_epoch_loss += source_loss.item()\n",
    "        \n",
    "        # ----- Target Domain Training -----\n",
    "        # Unpack target batch\n",
    "        target_images, target_prior_images, target_targets = target_batch\n",
    "        target_images = [img.to(device) for img in target_images]\n",
    "        target_prior_images = [img.to(device) for img in target_prior_images]\n",
    "        target_targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in target_targets]\n",
    "        \n",
    "        # Convert lists to tensors\n",
    "        target_images_tensor = torch.stack(target_images)\n",
    "        target_prior_images_tensor = torch.stack(target_prior_images)\n",
    "        \n",
    "        # Forward pass for target domain\n",
    "        target_losses = model(target_images_tensor, target_prior_images_tensor, target_targets)\n",
    "        \n",
    "        # Calculate total target loss\n",
    "        target_loss = 0.0\n",
    "        if 'loss_pal' in target_losses:\n",
    "            target_loss += target_losses['loss_pal']\n",
    "        if 'loss_reg' in target_losses:\n",
    "            target_loss += target_losses['loss_reg']\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        target_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        target_epoch_loss += target_loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_source_loss = source_epoch_loss / num_batches if num_batches > 0 else 0\n",
    "    avg_target_loss = target_epoch_loss / num_batches if num_batches > 0 else 0\n",
    "    \n",
    "    return avg_source_loss, avg_target_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training for a small number of epochs to test\n",
    "num_epochs = 2  # Set a small number for testing\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train for one epoch\n",
    "    source_loss, target_loss = train_epoch(\n",
    "        model, source_loader, target_loader, optimizer, device, epoch\n",
    "    )\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Source Loss: {source_loss:.4f}, Target Loss: {target_loss:.4f}\")\n",
    "    \n",
    "    # Adjust learning rate (after 50K iterations in the original paper)\n",
    "    if epoch == num_epochs // 2:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1\n",
    "            print(f\"Adjusted learning rate to {param_group['lr']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
